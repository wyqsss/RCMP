# -*- coding: utf-8 -*-
"""LAGAI Final Project Simon,Russell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oZLMhWONKdBULaTfXSXw8O4SekEs9UlH

Dependencies Installation
---
"""



# Commented out IPython magic to ensure Python compatibility.
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# This code creates a virtual display to draw game images on. 
# If you are running locally, just ignore it
import os
if type(os.environ.get("DISPLAY")) is not str or len(os.environ.get("DISPLAY"))==0:
  os.system("bash ../xvfb start")
#     %env DISPLAY=:1

import gym
import statistics
import tensorflow as tf
# from tensorflow import keras
import keras
from keras import layers
from keras.models import Sequential
from keras.layers import Dense
from collections import deque
import random
import copy
from stable_baselines3 import A2C
from stable_baselines3 import DQN
from stable_baselines3 import PPO
import pandas as pd

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

"""# Bootstrapped DQN + Uncertainty Aware Action Advising

## Main Implementation
"""

class RCMPDDQN: 
  def __init__(self, env, obs_continuous = True, num_heads = 4, minibatch_size = 8, head_chance = 0.3, \
               stride_length = 50, uncertainty_limit = 0.5, help_limit = 700, alpha = 0.01,\
               epsilon=0.2, gamma=0.8, buffer_size=2500):
    # Hyperparameters
    # --------------------
    # Environment (Assume box obs and discrete action)
    self.obs_continuous = obs_continuous
    if self.obs_continuous:  #Continuous Obs
      self.obs_size = env.observation_space.shape[0]
    else: #Discrete obs
      self.obs_size = env.observation_space.n
    self.num_actions = env.action_space.n
    self.num_heads = num_heads
    # Experience Replay / DQN Training
    self.minibatch_size = minibatch_size
    self.stride_length = stride_length
    self.alpha = alpha
    self.epsilon = epsilon # Set to 0 for bootstrapped exploration, >0 is epsilon greedy + bootstrapped exploration
    self.gamma = gamma
    self.buffer_size = buffer_size
    self.memory = deque([], maxlen = buffer_size)
    # RCMP / U3A
    self.env = env
    self.reward_history = list()
    self.uncertainty_history = list()
    self.ep_uncertainty_history = list()
    self.uncertainty_limit = uncertainty_limit
    self.help_remaining = help_limit
    self.head_chance = head_chance
    self.createNetworkEnsemble()

   
  def createNetworkEnsemble(self):
    # Create head networks in the form (head, target)
    self.networks = list()
    self.networks_original = list()
    
    for _ in range(self.num_heads):
      head = Sequential()
      # head.add(Dense(25, input_dim=self.obs_size, activation = 'relu', name = 'layer_1'))
      # head.add(Dense(25, activation='relu', name='layer_2'))
      head.add(Dense(self.num_actions, input_dim = self.obs_size, name = "layer_3"))
      head.compile(loss="mean_squared_error", optimizer=keras.optimizers.Adam(lr=self.alpha))

      target = Sequential()
      # target.add(Dense(25, input_dim=self.obs_size, activation = 'relu', name = 'layer_1'))
      # target.add(Dense(25, activation='relu', name='layer_2'))
      target.add(Dense(self.num_actions, input_dim = self.obs_size, name = "layer_3"))
      target.compile(loss="mean_squared_error", optimizer=keras.optimizers.Adam(lr=self.alpha))
      
      self.networks.append((head, target))
      self.networks_original.append((head,target))

  def helpAvailable(self):
    if self.help_remaining > 0:
      self.help_remaining -= 1
      return True
    return False

  def setDemonstrator(self, model):
    # param model: StableBaselines 3 Model
    self.demonstrator = copy.deepcopy(model)

  def demonstratorAction(self, state):
    # param self.demonstrator: StableBaselines 3 Model
    # Returns only action, not next hidden stat
    if self.obs_continuous:
      return self.demonstrator.predict(state, deterministic=True)[0][0]
    return self.demonstrator.predict(state[0], deterministic=True)[0][0]

  def resetWeights(self):
    for h in range(self.num_heads):
      self.networks[h][0].set_weights(self.networks_original[h][0].get_weights())
      self.networks[h][1].set_weights(self.networks_original[h][1].get_weights())
      
  
  def init_layer(self,layer):
    session = keras.backend.get_session()
    weights_initializer = tf.variables_initializer(layer.weights)
    session.run(weights_initializer)

  def train(self, num_epochs, num_steps):
    # Reset network weights
    # Data Collection
    total_steps = 0
    # Reset history lists
    self.reward_history = list()
    self.uncertainty_history = list()
    self.ep_uncertainty_history = list()

    for ep in range(num_epochs):
      print("Start of Epoch help remaining", self.help_remaining)
      print("Epoch = ", ep, end = "|")
      reward_total = 0
      ep_steps = 0
      ep_avg_uncertainty = 0
      # Set random head as the "core" decider for this epoch
      coreN, targetN = random.choice(self.networks)
      # Initialize state
      state = self.env.reset()
      state = np.resize(state, (1, self.obs_size))
      for step in range(num_steps):
        # ------------------------------RCMP--------------------------------
        # Calculate uncertainty mu
        variance_sum = 0
        # Sum over actions
        for a in range(self.num_actions):
          q_values = list()
          # Create list of Q(s,a) values over heads
          for h in range(self.num_heads):
            q_value = np.array(self.networks[h][0].predict(state))[0]
            # print(f"qval_is {q_value}")
            q_values.append(q_value[a])
          # Add variance to total variance
          # print(q_values)
          variance_sum += np.var(q_values)
        # Divide summed variance by num_actions
        mu = variance_sum / self.num_actions
        print(f"uncertainty is {mu}")
        ep_avg_uncertainty+= mu
        # Data Collection
        self.uncertainty_history.append(mu)
        # Action Advising
        used_help = False
        if mu > self.uncertainty_limit and self.helpAvailable(): # Take action demonstrator would take
          action =  self.demonstratorAction(state)
          print("use demostration")
          used_help = True
        else: # Take action from normal policy (eg. Epsilon Greedy)
          if random.random() < self.epsilon:
            action = self.env.action_space.sample()
          else:
            # Core Action
            action = np.argmax(coreN.predict(state))  #这一步与论文不一致，论文采取的是平均
        # -----------------------Step The Environment------------------------
        next_state, reward, done, _ = self.env.step(action)
        ep_steps += 1
        next_state = np.resize(next_state, (1, self.obs_size))
        reward_total += reward
        # Store the transition
        self.memory.append((state, action, reward, next_state, done))
        if done:
          total_steps+=1
          break
        # Experience Replay Buffer too small
        if len(self.memory) < self.minibatch_size:
          total_steps += 1
          state = next_state
          continue
        # ------------------------Experience Replay--------------------------
        # Sample a minibatch of transitions
        minibatch = random.sample(self.memory, self.minibatch_size-1)
        minibatch.append((state,action,reward,next_state,done))
        # Train all the heads
        for h in range(self.num_heads):
          # Create tensors that contain minibatch_size row vectors
          targetsH = np.array([[0.0]*self.num_actions]*self.minibatch_size)
          targetsH = np.reshape(targetsH, (self.minibatch_size, self.num_actions))
          statesH = np.array([[0.0]*self.obs_size]*self.minibatch_size)
          statesH = np.reshape(statesH, (self.minibatch_size,self.obs_size))
          
          # Creating the target and prediction tensors for batch training
          i = 0
          for (s, a, r, ns, d) in minibatch:
            statesH[i] = s[0]
            targetsH[i] = np.array(self.networks[h][0].predict(s))
            # If we should train on this SA pair form minibatch
            if random.random() > self.head_chance: 
              targetsH[i][a] = r
              if not d:
                prediction = np.array(self.networks[h][1].predict(ns))
                maxQIndex = np.argmax(prediction)
                maxQ = prediction[0][maxQIndex]
                targetsH[i][a] += self.gamma * maxQ
            i += 1
            
          # Train head on minibatch
          self.networks[h][0].train_on_batch(statesH, targetsH)
          if total_steps % self.stride_length == 0:
            # Copy head newtork weights to corresponding target network
            self.networks[h][1].set_weights(self.networks[h][0].get_weights()) 
        
        total_steps += 1
        # Update the state
        state = next_state
      
      # END FOR STATE
      # -------------------------------------------
      # Data Collection
      self.reward_history.append(reward_total)
      self.ep_uncertainty_history.append(ep_avg_uncertainty/step)
      print(f"epoch step is {ep_steps}")
      # Print the reward
      print("Reward = {}, Average 50 Reward = {:.2f}".format(reward_total, statistics.mean(self.reward_history[-50:])))
    # END FOR EPOCH
    # ------------------------------------------
    self.env.reset()
    rList = []
    for i in range(1000):
      ## Reset environment and get first new observation
      s = self.env.reset()  # observation is state, integer 0 ~ 15
      rAll = 0
      for j in range(99):  # step index, maximum step is 99
          ## Choose an action by greedily (with e chance of random action) from the Q-network
          allQ = self.networks[0][0].predict(s)
          a = np.argmax(allQ, 1)  # no epsilon, only greedy for testing

          ## Get new state and reward from environment
          s1, r, d, _ = self.env.step(a[0])
          rAll += r
          s = s1
          ## Reduce chance of random action if an episode is done.
          if d: break

      print('Testing  | Episode: {}/{}  | Episode Reward: {:.4f} | Running Time: {:.4f}'.format(i, 1000, rAll))
      rList.append(rAll)
    print("正确率: " + str(sum(rList) / 1000 * 100) + "%")

    return reward_total

# Create the environment
# List of env names: "FrozenLake-v1", "CartPole-v1", "MountainCar-v0" 
env_name = "FrozenLake-v1"
env = gym.make(env_name)
# Load the demonstrator model
PPO_model_name = "ppo_frozenlake_1e6"
model = PPO('MlpPolicy', env, verbose=1)
model = PPO.load(PPO_model_name)
model.set_env(env)

RCMP_model = RCMPDDQN(env = env, obs_continuous=False, num_heads=5, help_limit=0, uncertainty_limit=0.11, gamma=0.99, minibatch_size=64, alpha=1e-3, buffer_size=2000)
RCMP_model.setDemonstrator(model)
num_ep = 1000
num_steps = 99
num_runs = 1
run_rewards = list()
ep_uncertainties = []
for run in range(num_runs):
  RCMP_model.train(num_ep, num_steps)
  RCMP_model.resetWeights()
  run_rewards.append(RCMP_model.reward_history)

  dataframe = pd.DataFrame({f"heads{RCMP_model.num_heads}": RCMP_model.ep_uncertainty_history})
  dataframe.to_csv(f"heads{RCMP_model.num_heads}.csv",sep=',')


  ep_uncertainties.append(RCMP_model.ep_uncertainty_history)
  print("finish trian")
  # print(f"uncertrainty is {RCMP_model.ep_uncertainty_history}")
  # Plot each run results
  
  # Plot Uncertainty by timestep
  # fig, ax = plt.subplots(1)
  # total_steps = list(range(len(RCMP_model.uncertainty_history)))
  # ax.plot(total_steps, RCMP_model.uncertainty_history, label = "Timestep Uncertainty")
  # ax.legend()
  # plt.title("Uncertainty by timestep")

  # # Plot Uncertainty by epoch
  # fig2, ax2 = plt.subplots(1)
  # ax2.plot(list(range(num_ep)), RCMP_model.ep_uncertainty_history, label = "Epoch Average Uncertainty")
  # ax2.legend()
  # plt.title("Uncertainty by epoch")
  # plt.savefig("ep_uncertainty")

  # # Plot Reward by epoch
  # fig3, ax3 = plt.subplots(1)
  # ax3.plot(list(range(num_ep)), RCMP_model.reward_history, label = "Epoch Reward")
  # ax3.legend()
  # plt.title("Reward by Epoch")


# Plot avg reward
# fig4,ax4 = plt.subplots(1)
# avg_reward = np.mean(list(run_rewards[i] for i in range(num_runs)), axis = 0)
# ax4.plot(list(range(num_ep)),avg_reward, label = str(num_runs) + " Run Reward Average")
# plt.title("Avg Reward by Epoch")

print("plot------")
for ep_uncertainty in ep_uncertainties:
  plt.plot(list(range(num_ep)), ep_uncertainty)

plt.title("Uncertainty by epoch")
plt.savefig(f"heads{RCMP_model.num_heads}_ep_uncertainty")